# Proyecto final [Galaxian] üöÄ

### Programador
Marco Carbajal (23025)
car23025@uvg.edu.gt

> Universidad del Valle de Guatemala - Aprendizaje por refuerzo (Secci√≥n 21)

## Estructura del proyecto

```
proyecto_galaxian/
‚îÇ
‚îú‚îÄ‚îÄ setup.py               # Setup autom√°tico (Lo primero que se debe ejecutar)
‚îú‚îÄ‚îÄ init_env.py            # Inicializaci√≥n de entornos ALE
‚îú‚îÄ‚îÄ test_environment.py    # Prueba r√°pida (generado por `setup.py`)
‚îú‚îÄ‚îÄ requirements.txt       # Dependencias
‚îÇ
‚îú‚îÄ‚îÄ train_dqn.py          # Entrenamiento DQN
‚îú‚îÄ‚îÄ train_a2c.py          # Entrenamiento A2C
‚îú‚îÄ‚îÄ analyze.py            # An√°lisis de rendimiento
‚îú‚îÄ‚îÄ play_final.py         # Evaluaci√≥n final
‚îÇ
‚îú‚îÄ‚îÄ utils.py              # Utilidades compartidas
‚îú‚îÄ‚îÄ policy_wrapper.py     # Cargar modelos entrenados
‚îÇ
‚îú‚îÄ‚îÄ modelos/              # Modelos guardados
‚îú‚îÄ‚îÄ graficas/             # Gr√°ficas de entrenamiento
‚îú‚îÄ‚îÄ videos_entrenamiento/ # Videos peri√≥dicos
‚îî‚îÄ‚îÄ grabaciones/          # Videos finales para entrega
```

## Instalaci√≥n

**IMPORTANTE: Ejecuta el setup autom√°tico primero**

```bash
# Setup autom√°tico
python setup.py

# Esto instalar√° todo en el orden correcto y verificar√° que funcione
```

El script de setup har√°:
1. Crear la estructura de directorios (si todav√≠a no est√°n creados)
2. Instalar dependencias en el orden correcto
3. Registrar entornos de ALE autom√°ticamente
4. Verificar que todo funcione
5. Crear un script de prueba

### Verificaci√≥n post-instalaci√≥n

```bash
# Prueba r√°pida
python test_environment.py

# Si aparece "‚úì ¬°Todo funciona correctamente!", est√° listo
```

## Gu√≠a de uso

### Fase 1: Entrenamiento inicial (Comparaci√≥n)

1. **Entrenar DQN** (1000-2000 episodios para comparaci√≥n):
```bash
python train_dqn.py
```

2. **Entrenar A2C** (1000-2000 episodios para comparaci√≥n):
```bash
python train_a2c.py
```

3. **Comparar ambos m√©todos**:
```bash
python compare_methods.py
```

Este script mostrar√° cu√°l m√©todo funciona mejor y generar√°:
- `graficas/comparacion_metodos.png` - Comparaci√≥n visual
- `graficas/comparacion.json` - Estad√≠sticas detalladas

### Fase 2: Entrenamiento final

Una vez elegido el mejor m√©todo, contin√∫a con el entrenamiento:

**Si result√≥ ser DQN:**
```bash
# Editar train_dqn.py:
# - Cambiar 'resume_training': True
# - Aumentar 'n_episodes': 5000 (o m√°s)

python train_dqn.py
```

**Si result√≥ ser A2C:**
```bash
# Editar train_a2c.py:
# - Cambiar 'resume_training': True
# - Aumentar 'n_episodes': 5000 (o m√°s)

python train_a2c.py
```

### Fase 3: Evaluaci√≥n final (para la entrega)

```bash
# Editar play_final.py:
# - Configurar 'model_path' con el mejor modelo
# - Configurar 'method' ('dqn' o 'a2c')

python play_final.py
```

Esto generar√° 3 videos en `grabaciones/` con el formato requerido.

## Configuraci√≥n de hiperpar√°metros

### DQN (train_dqn.py)

```python
HIPERPARAMETROS = {
    'n_episodes': 5000,          # Episodios totales
    'learning_rate': 0.00025,    # Tasa de aprendizaje
    'gamma': 0.99,               # Factor de descuento
    'epsilon_start': 1.0,        # Exploraci√≥n inicial
    'epsilon_end': 0.01,         # Exploraci√≥n final
    'buffer_size': 100000,       # Tama√±o del replay buffer
    'batch_size': 32,            # Tama√±o del batch
    'target_update_freq': 1000,  # Actualizar target network
}
```

### A2C (train_a2c.py)

```python
HIPERPARAMETROS = {
    'n_episodes': 5000,          # Episodios totales
    'learning_rate': 0.0007,     # Tasa de aprendizaje
    'gamma': 0.99,               # Factor de descuento
    'n_steps': 5,                # N-step returns
    'entropy_coef': 0.01,        # Coeficiente de entrop√≠a
    'value_coef': 0.5,           # Coeficiente del critic
}
```

## Monitoreo del entrenamiento

Durante el entrenamiento, se muestra algo como:
```
Ep  100 | Reward:    340 | Avg100:  285.5 | Eps: 0.900 | Loss: 0.1234
üíæ Checkpoint guardado: modelos/dqn_ep100.pth
```

- **Reward**: Puntuaci√≥n del episodio actual
- **Avg100**: Promedio de √∫ltimos 100 episodios
- **Eps**: Epsilon actual (solo DQN)
- **Loss**: Loss promedio del episodio

### Checkpoints autom√°ticos (cada 250 episodios):

- `modelos/[method]_ep250.pth`, `[method]_ep500.pth`, etc. - Checkpoints espec√≠ficos
- `modelos/[method]_checkpoint.pth` - √öltimo checkpoint (para resumir)
- `modelos/[method]_best.pth` - Mejor modelo hasta ahora
- `modelos/[method]_final.pth` - Modelo final del entrenamiento

### Videos de evaluaci√≥n (cada 100 episodios):

- `videos_entrenamiento/ep100/` - video de evaluaci√≥n
- `videos_entrenamiento/ep200/` - video de evaluaci√≥n
- etc.

Estos videos permiten **ver visualmente c√≥mo mejora el agente** a lo largo del tiempo.

### Gr√°ficas generadas autom√°ticamente:

**Gr√°fica completa de m√©tricas** (`[method]_metrics.png`):
- Recompensas por episodio con media m√≥vil
- Loss de entrenamiento
- Decay de epsilon (DQN) o pasos por episodio
- Estad√≠sticas acumuladas (media y m√°ximo)

**Estad√≠sticas JSON** (`[method]_stats.json`):
```json
{
  "total_episodes": 1000,
  "best_reward": 1300.0,
  "final_avg_100": 750.0,
  "final_epsilon": 0.01
}
```

## Sistema de Checkpoints

Los checkpoints se guardan autom√°ticamente cada 250 episodios:

**Nomenclatura de archivos**:
- `[method]_ep100.pth` - Checkpoint espec√≠fico del episodio 100
- `[method]_ep200.pth` - Checkpoint espec√≠fico del episodio 200
- `[method]_checkpoint.pth` - √öltimo checkpoint (para `resume_training`)
- `[method]_best.pth` - Mejor modelo hasta el momento
- `[method]_final.pth` - Modelo final

**Contenido de cada checkpoint**:
- Estado del modelo (`model_state_dict`)
- Estado del optimizador (`optimizer_state_dict`)
- N√∫mero de episodio (`episode`)
- Historial de recompensas (`rewards`)
- Historial de losses (`losses`)
- Historial de epsilons (`epsilons` - solo DQN)
- Mejor recompensa (`best_reward`)

Para continuar un entrenamiento interrumpido:
```python
'resume_training': True  # Cambiar en HIPERPARAMETROS
```

## Sistema de videos

### Videos durante el entrenamiento
Cada 100 episodios, se graban un videos de evaluaci√≥n:

```
videos_entrenamiento/
  ‚îú‚îÄ‚îÄ ep100/
  ‚îÇ   ‚îú‚îÄ‚îÄ dqn_ep100_score510.mp4
  ‚îÇ   ‚îî‚îÄ‚îÄ a2c_ep100_score470.mp4
  ‚îú‚îÄ‚îÄ ep200/
  ‚îú‚îÄ‚îÄ ep.../
  ‚îî‚îÄ‚îÄ ep1000/
```

### Analizar progreso
```bash
python analyze.py
```

**Opciones del men√∫**:
1. **Resumen general** - Estado completo del proyecto
2. **Analizar checkpoints (DQN)** - Tabla de progreso
3. **Analizar checkpoints (A2C)** - Tabla de progreso
4. **Analizar videos (DQN)** - Progreso visual por episodio
5. **Analizar videos (A2C)** - Progreso visual por episodio
6. **Comparar m√©todos** - Decisi√≥n de cu√°l usar

## Acciones disponibles en galaxian
```
0 -> NOOP       - No hacer nada
1 -> FIRE       - Disparar
2 -> RIGHT      - Mover derecha
3 -> LEFT       - Mover izquierda
4 -> RIGHTFIRE  - Mover derecha + disparar
5 -> LEFTFIRE   - Mover izquierda + disparar
```

## Workflow simplificado

```bash
# 1. SETUP (una vez)
python setup.py
python test_environment.py

# 2. ENTRENAMIENTO INICIAL (Comparaci√≥n)
python train_dqn.py      # Entrenar ~2000 episodios
python train_a2c.py      # Entrenar ~2000 episodios

# 3. AN√ÅLISIS Y DECISI√ìN
python analyze.py        # Opci√≥n 6: Comparar m√©todos

# 4. ENTRENAMIENTO FINAL
# Editar train_[metodo_elegido].py: resume_training=True, n_episodes=5000 (o m√°s)
python train_[metodo_elegido].py      # (o el m√©todo elegido)

# 5. MONITOREO (durante entrenamiento)
python analyze.py        # Ver progreso peri√≥dicamente

# 6. EVALUACI√ìN FINAL
# Editar play_final.py: model_path='modelos/[metodo_elegido]_best.pth'
python play_final.py     # Generar videos para entrega
```