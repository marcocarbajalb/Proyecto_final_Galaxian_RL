# Proyecto final [Galaxian] üöÄ

**Desarrollado por:** Marco Carbajal (car23025@uvg.edu.gt)

> Universidad del Valle de Guatemala - Aprendizaje por refuerzo (Secci√≥n 21)

---

## Estructura del proyecto

```
proyecto_galaxian/
‚îÇ
‚îú‚îÄ‚îÄ setup.py                     # Configuraci√≥n autom√°tica del entorno
‚îú‚îÄ‚îÄ init_env.py                  # Inicializaci√≥n de entornos ALE
‚îú‚îÄ‚îÄ test_environment.py          # Prueba r√°pida del entorno
‚îú‚îÄ‚îÄ requirements.txt             # Dependencias del proyecto
‚îÇ
‚îú‚îÄ‚îÄ train_dqn.py                 # Entrenamiento con Deep Q-Network
‚îú‚îÄ‚îÄ train_a2c.py                 # Entrenamiento con Advantage Actor-Critic
‚îú‚îÄ‚îÄ experiment_architectures.py  # Experimentaci√≥n con variantes de arquitectura
‚îú‚îÄ‚îÄ analyze.py                   # An√°lisis de rendimiento y comparaci√≥n
‚îú‚îÄ‚îÄ play_final.py                # Evaluaci√≥n final y generaci√≥n de videos
‚îÇ
‚îú‚îÄ‚îÄ utils.py                     # Utilidades compartidas (preprocesamiento, replay buffer)
‚îú‚îÄ‚îÄ policy_wrapper.py            # Carga de modelos entrenados
‚îÇ
‚îú‚îÄ‚îÄ modelos/                     # Modelos guardados (.pth)
‚îú‚îÄ‚îÄ graficas/                    # Gr√°ficas de entrenamiento y comparaci√≥n
‚îú‚îÄ‚îÄ experimentos/                # Resultados de experimentos con arquitecturas
‚îú‚îÄ‚îÄ videos_entrenamiento/        # Videos peri√≥dicos durante entrenamiento
‚îî‚îÄ‚îÄ grabaciones/                 # Videos finales para entrega
```

---

## Metodolog√≠a y proceso de desarrollo

### Fase 1: Configuraci√≥n del entorno

El proyecto inici√≥ con la configuraci√≥n del entorno de desarrollo utilizando `setup.py`, que automatiz√≥:
- Instalaci√≥n de dependencias (Gymnasium, ALE-Py, PyTorch, OpenCV)
- Registro de entornos de Arcade Learning Environment [ALE]
- Verificaci√≥n de que el entorno Galaxian funcionara correctamente

```bash
python setup.py
python test_environment.py  # Verificaci√≥n exitosa (se genera autom√°ticamente)
```

### Fase 2: Entrenamiento comparativo inicial (DQN vs A2C)

Se implementaron dos de los algoritmos de aprendizaje por refuerzo aprendidos en el curso para comparar su efectividad en Galaxian:

#### 2.1 Deep Q-Network (DQN)
Implementaci√≥n con las siguientes caracter√≠sticas:
- **Arquitectura:** 3 capas convolucionales + 2 capas fully connected
- **Replay buffer:** 100,000 experiencias
- **Epsilon-greedy:** Exploraci√≥n que decae linealmente de 1.0 a 0.01
- **Target network:** Actualizada cada 1,000 pasos

**Hiperpar√°metros utilizados:**
```python
{
    'n_episodes': 2000,
    'learning_rate': 0.0001,
    'gamma': 0.99,
    'epsilon_decay': 1700,
    'batch_size': 32,
    'target_update_freq': 1000
}
```

#### 2.2 Advantage Actor-Critic (A2C)
Implementaci√≥n caracterizada por:
- **Arquitectura dual:** Actor y Critic comparten capas convolucionales
- **N-step returns:** C√°lculo de retornos con n=5 pasos
- **Entrop√≠a:** Coeficiente de 0.01 para fomentar exploraci√≥n

**Hiperpar√°metros utilizados:**
```python
{
    'n_episodes': 2000,
    'learning_rate': 0.0005,
    'gamma': 0.99,
    'n_steps': 5,
    'entropy_coef': 0.01,
    'value_coef': 0.5
}
```

#### 2.3 Resultados de la comparaci√≥n

Despu√©s de entrenar ambos m√©todos por 2,000 episodios, se utiliz√≥ `analyze.py` para generar una comparaci√≥n:

```bash
python analyze.py  # Opci√≥n 6: Comparar m√©todos
```

**Conclusi√≥n:** DQN demostr√≥ ser superior, con una ventaja consistente en el promedio de recompensas de los √∫ltimos 100 episodios. Las gr√°ficas comparativas en `graficas/comparacion_metodos.png` muestran claramente esta diferencia de desempe√±o.

### Fase 3: Experimentaci√≥n con arquitecturas

Una vez seleccionado DQN como el m√©todo principal, se procedi√≥ a explorar variantes de arquitectura neuronal para optimizar el rendimiento.

#### 3.1 Arquitecturas evaluadas

Se implement√≥ `experiment_architectures.py` para probar cuatro variantes:

1. **Original (Baseline):** 3 capas convolucionales + 2 capas fully connected
2. **Deeper:** 4 capas convolucionales + 3 capas connected (m√°s profundidad)
3. **Wider:** 3 capas convolucionales + 2 capas connected con m√°s filtros
4. **Dueling:** Arquitectura Dueling DQN con streams separados para valor y ventaja

Cada variante fue entrenada por 500 episodios para una comparaci√≥n r√°pida:

```bash
python experiment_architectures.py
```

#### 3.2 Resultados de la experimentaci√≥n

El an√°lisis de resultados (disponible en `experimentos/comparacion/architectures_comparison.png`) revel√≥ que:

- **La arquitectura baseline mantuvo el mejor desempe√±o**
- Las arquitecturas m√°s complejas no proporcionaron mejoras significativas
- El balance entre capacidad y complejidad de la arquitectura original fue √≥ptimo

**Decisi√≥n:** Se mantuvo la arquitectura original de 3 capas convolucionales y 2 capas fully connected para el entrenamiento final. As√≠ pues, los 2,000 episodios de entrenamiento previos con DQN pudieron ser aprovechados para continuar entrenando al agente.

### Fase 4: Entrenamiento extensivo

Con la arquitectura y m√©todo seleccionados, se procedi√≥ al entrenamiento extensivo del agente.

#### 4.1 Configuraci√≥n del entrenamiento continuo

Se modific√≥ `train_dqn.py` para continuar desde el checkpoint existente:

```python
HIPERPARAMETROS = {
    'resume_training': True,
    'checkpoint_path': 'modelos/dqn_checkpoint.pth',
    'n_episodes': 7212,  # Extensi√≥n del entrenamiento
    # ... resto de hiperpar√°metros sin cambios
}
```

> Realmente el agente no fue entrenado desde el episodio 2,000 hasta el episodio 7,212 de corrido, se fue haciendo por partes. Sin embargo, esta es una simplificaci√≥n del proceso. 

#### 4.2 Monitoreo del progreso

Durante el entrenamiento, se generaron autom√°ticamente:
- **Checkpoints cada 250 episodios:** Permiten reanudar el entrenamiento
- **Videos de evaluaci√≥n cada 100 episodios:** Muestran la evoluci√≥n visual del agente
- **Gr√°ficas de m√©tricas:** Recompensas, loss, epsilon, estad√≠sticas acumuladas

El sistema de checkpoints guard√≥:
- `dqn_ep250.pth`, `dqn_ep500.pth`, etc. - Checkpoints espec√≠ficos
- `dqn_checkpoint.pth` - √öltimo estado para continuar
- `dqn_best.pth` - **Mejor episodio del entrenamiento**
- `dqn_final.pth` - Estado final tras 7,212 episodios

#### 4.3 Observaciones del entrenamiento

El an√°lisis de las gr√°ficas de entrenamiento (`graficas/dqn_metrics.png`) revel√≥:
- **Convergencia gradual** hacia mejores pol√≠ticas en los primeros 5,000 episodios
- **M√°ximo rendimiento** alcanzado cerca del episodio 6,500-7,000
- **Degradaci√≥n en episodios finales:** La media m√≥vil comenz√≥ a descender despu√©s del episodio 7,000

Esta degradaci√≥n sugiere posible *catastrophic forgetting* en las √∫ltimas etapas del entrenamiento. 

### Fase 5: Selecci√≥n del modelo final

Para la evaluaci√≥n final, se tom√≥ una decisi√≥n cr√≠tica sobre qu√© modelo utilizar.

#### 5.1 An√°lisis de candidatos

Se evaluaron dos opciones principales:
- `dqn_final.pth` - Estado final tras 7,212 episodios
- `dqn_best.pth` - Episodio con mayor puntuaci√≥n durante el entrenamiento

#### 5.2 Decisi√≥n fundamentada

**Modelo seleccionado:** `dqn_best.pth`

**Justificaci√≥n**:
1. Representa el **mejor desempe√±o individual** alcanzado durante todo el entrenamiento
2. Corresponde a un episodio cercano al final, pero **antes de la degradaci√≥n observada**
3. La ca√≠da en la media de los √∫ltimos episodios sugiere que el modelo final no era √≥ptimo

#### 5.3 Generaci√≥n de videos finales

Se configur√≥ `play_final.py` con el modelo seleccionado:

```python
CONFIG = {
    'model_path': 'modelos/dqn_best.pth',
    'method': 'dqn',
    'n_episodes': 3,
    'output_dir': 'grabaciones',
}
```

Ejecuci√≥n:
```bash
python play_final.py
```

Esto gener√≥ 3 videos de evaluaci√≥n en `grabaciones/` con el formato requerido: `car23025_[timestamp]_[score].mp4`. La puntuaci√≥n con la que compet√≠ (la m√°s alta de los 3 videos generados) fue 9230 puntos. 

---

## Arquitectura del modelo final

### Red Neuronal Convolucional (CNN)

```
Entrada: Stack de 4 frames (4 √ó 84 √ó 84)
    ‚Üì
Conv2D(4‚Üí32, kernel=8, stride=4) + ReLU
    ‚Üì
Conv2D(32‚Üí64, kernel=4, stride=2) + ReLU
    ‚Üì
Conv2D(64‚Üí64, kernel=3, stride=1) + ReLU
    ‚Üì
Flatten
    ‚Üì
Linear(3136‚Üí512) + ReLU
    ‚Üì
Linear(512‚Üí6)  [Q-values para 6 acciones]
```

**Total de par√°metros:** ~1.6M

### Preprocesamiento de frames

1. **Conversi√≥n a escala de grises:** Reducci√≥n de dimensionalidad
2. **Redimensionamiento:** 210√ó160√ó3 ‚Üí 84√ó84√ó1
3. **Normalizaci√≥n:** Valores entre 0 y 1
4. **Frame stacking:** Stack de los √∫ltimos 4 frames para capturar movimiento

### Acciones disponibles en Galaxian

```
0 ‚Üí NOOP       - Sin acci√≥n
1 ‚Üí FIRE       - Disparar
2 ‚Üí RIGHT      - Mover derecha
3 ‚Üí LEFT       - Mover izquierda
4 ‚Üí RIGHTFIRE  - Mover derecha + disparar
5 ‚Üí LEFTFIRE   - Mover izquierda + disparar
```